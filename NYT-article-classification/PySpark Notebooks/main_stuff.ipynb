{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "sc = SparkContext('local')\n",
    "line=sc.wholeTextFiles(path=\"../NYT articles/\")\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "a=line.collect()\n",
    "for i in range(len(a)):\n",
    "    a[i]=list(a[i])\n",
    "    a[i][0]=re.sub(r'\\d+', '',a[i][0])\n",
    "    a[i]=tuple(a[i])\n",
    "rdd = sc.parallelize(a)\n",
    "people = rdd.map(lambda x: Row(label=x[0], sentence=(x[1])))\n",
    "sqlContext = SQLContext(sc)\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "regexTokenizer = RegexTokenizer(inputCol= 'sentence', outputCol='words')\n",
    "regexTokenized = regexTokenizer.transform(schemaPeople)\n",
    "#regexTokenized.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, sentence: string]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is read using wholeTextFiles. This function can be used to read all the files present in the given directory. It returns a list of tuples. Each tuple contains address of file and data. Data is cleaned and seperated by tokenizer. Stopwords, digits and symbols are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               label|            sentence|               words|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "sw=remover.transform(regexTokenized)\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(sw)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF is applied on the clean data which helps to get features for the words present in the file. It returns importance of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           label|            features|\n",
      "+----------------+--------------------+\n",
      "|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import re\n",
    "from pyspark.sql.types import *\n",
    "name='label'\n",
    "# udf = UserDefinedFunction(lambda x: re.sub('file:/home/shreyas/Lab3data/try/','',str(x)), StringType())\n",
    "# new_df = rescaledData.select(*[udf(column).alias(name) if column == name else column for column in rescaledData.columns])\n",
    "udf = UserDefinedFunction(lambda x: re.sub('.txt','',str(x)), StringType())\n",
    "new_df = rescaledData.select(*[udf(column).alias(name) if column == name else column for column in rescaledData.columns])\n",
    "\n",
    "udf1 = UserDefinedFunction(lambda x: re.sub('file:/home/hxd/Documents/NYT-article-classification/','',str(x)), StringType())\n",
    "re = new_df.select(*[udf1(column).alias(name) if column == name else column for column in new_df.columns])\n",
    "\n",
    "re.select(\"label\",\"features\").show(2)\n",
    "final=re.select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|label           |\n",
      "+----------------+\n",
      "|NYT articles/NBA|\n",
      "|NYT articles/NBA|\n",
      "+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing only labels\n",
    "re.select(\"label\").show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The filename returned by wholeTextFile is used as label. It returns the absolute address of file. So the absolute address is removed and only label part is kept.(We have saved files with the labelname.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in train and test set\n",
    "\n",
    "###### Cleaned data is divided into train and test set by the ratio of 4:1\n",
    "\n",
    "## Convert string labels to integer\n",
    "\n",
    "##### Every label has been assigned by a integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|              label|            features|categoryIndex|\n",
      "+-------------------+--------------------+-------------+\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|NYT articles/readme|(20,[4,5,8,11],[0...|          2.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"categoryIndex\")\n",
    "#indexed = labelIndexer.fit(final).transform(final)\n",
    "#indexed.show()\n",
    "labelIndexer= stringIndexer.fit(final)\n",
    "indexed= labelIndexer.transform(final)\n",
    "indexed.show()\n",
    "\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(indexed)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(train, test) = indexed.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, features: vector, categoryIndex: double]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression using pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------+\n",
      "|              label|            features|categoryIndex|\n",
      "+-------------------+--------------------+-------------+\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|NYT articles/readme|(20,[4,5,8,11],[0...|          2.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|   NYT articles/NBA|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Coefficients: \n",
      "DenseMatrix([[ 1.1532269 ,  1.87450437,  3.13583258,  1.11953701,  2.92980597,\n",
      "               2.70044606,  2.00897262,  1.28125068,  2.3270106 ,  2.7447127 ,\n",
      "               0.63893222,  2.52434939,  1.4478468 ,  1.17025083,  1.71294051,\n",
      "               2.54091741,  1.01117895,  1.22806642,  1.80915939,  1.52137778],\n",
      "             [-0.84056655, -1.46192326, -2.23162823, -0.82701189, -2.25047732,\n",
      "              -2.00315508, -1.45533191, -0.95835419, -1.70268027, -2.01453219,\n",
      "              -0.45882979, -1.87871835, -1.08529674, -0.87559752, -1.32707835,\n",
      "              -1.82366957, -0.67095862, -0.90099512, -1.35260323, -1.18372957],\n",
      "             [-0.31266035, -0.41258111, -0.90420435, -0.29252512, -0.67932866,\n",
      "              -0.69729099, -0.55364072, -0.32289649, -0.62433033, -0.73018051,\n",
      "              -0.18010243, -0.64563104, -0.36255006, -0.29465331, -0.38586217,\n",
      "              -0.71724785, -0.34022032, -0.32707131, -0.45655616, -0.33764821]])\n",
      "Intercept: [-0.5982227633417428,3.184416644468646,-2.586193881126903]\n",
      "+----------+--------------------+----------------+\n",
      "|prediction|            features|  predictedLabel|\n",
      "+----------+--------------------+----------------+\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|NYT articles/NBA|\n",
      "+----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.0555556 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "indexed.show()\n",
    "lr = LogisticRegression(labelCol=\"categoryIndex\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(train)\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinelr = Pipeline(stages=[labelIndexer, featureIndexer, lr, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "train2 = train.drop(col(\"categoryIndex\"))\n",
    "lrModel = pipelinelr.fit(train2)\n",
    "\n",
    "test2 = test.drop(col(\"categoryIndex\"))\n",
    "predictions =lrModel.transform(test2)\n",
    "\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"features\",\"predictedLabel\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|  predictedLabel|           label|            features|\n",
      "+----------------+----------------+--------------------+\n",
      "|NYT articles/NBA|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "|NYT articles/NBA|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "|NYT articles/NBA|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "|NYT articles/NBA|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "|NYT articles/NBA|NYT articles/NBA|(20,[0,1,2,3,4,5,...|\n",
      "+----------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.0555556\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_0f7050258272, numTrees=10, numClasses=3, numFeatures=20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(final)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(final)\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinerf = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "rfmodel = pipelinerf.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = rfmodel.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = rfmodel.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+\n",
      "|              label|  predictedLabel|\n",
      "+-------------------+----------------+\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|   NYT articles/NBA|NYT articles/NBA|\n",
      "|NYT articles/readme|NYT articles/NBA|\n",
      "+-------------------+----------------+\n",
      "\n",
      "Test set accuracy = 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#import LabeledPoint\n",
    "\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",labelCol=\"categoryIndex\")\n",
    "\n",
    "# train the model\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinenb = Pipeline(stages=[labelIndexer, featureIndexer, nb, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "nbmodel = pipelinenb.fit(train)\n",
    "\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = nbmodel.transform(test)\n",
    "predictions.select(\"label\",\"predictedLabel\").show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying classifiers on the validation sets\n",
    "#### A validation set is being used. We used 'trump' as the topic for validation set which we though might get classified in Diplomacy and technology considering recent activities. We have got reasonably satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "+----------+--------------------+\n",
      "|prediction|      predictedLabel|\n",
      "+----------+--------------------+\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       1.0|NYT articles/Note...|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "|       0.0|    NYT articles/NBA|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Naive Bayes\n",
      "+----------+----------------+\n",
      "|prediction|  predictedLabel|\n",
      "+----------+----------------+\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "|       0.0|NYT articles/NBA|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validation Sets:\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "line=sc.wholeTextFiles(\"../Validation set\")\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "a=line.collect()\n",
    "\n",
    "rdd = sc.parallelize(a)\n",
    "people = rdd.map(lambda x: Row(sentence=(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(schemaPeople)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "sw=remover.transform(regexTokenized)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(sw)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "valData = idfModel.transform(featurizedData)\n",
    "#rescaledData.show(truncate=False)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "predictionsvalrf = rfmodel.transform(valData)\n",
    "predictionsvalrf.select('prediction','predictedLabel').show()\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "predictionsvalnb = nbmodel.transform(valData)\n",
    "predictionsvalnb.select('prediction','predictedLabel').show()\n",
    "\n",
    "# print(\"Logistic regression\")\n",
    "# predictionsvallr = lrModel.transform(valData)\n",
    "# predictionsvallr.select('prediction','predictedLabel').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
